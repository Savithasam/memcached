        -:    0:Source:slabs.c
        -:    0:Graph:slabs.gcno
        -:    0:Data:slabs.gcda
        -:    0:Runs:119
        -:    0:Programs:1
        -:    1:/* -*- Mode: C; tab-width: 4; c-basic-offset: 4; indent-tabs-mode: nil -*- */
        -:    2:/*
        -:    3: * Slabs memory allocation, based on powers-of-N. Slabs are up to 1MB in size
        -:    4: * and are divided into chunks. The chunk sizes start off at the size of the
        -:    5: * "item" structure plus space for a small key and value. They increase by
        -:    6: * a multiplier factor from there, up to half the maximum slab size. The last
        -:    7: * slab size is always 1MB, since that's the maximum item size allowed by the
        -:    8: * memcached protocol.
        -:    9: */
        -:   10:#include "memcached.h"
        -:   11:#include <sys/mman.h>
        -:   12:#include <sys/stat.h>
        -:   13:#include <sys/socket.h>
        -:   14:#include <sys/resource.h>
        -:   15:#include <fcntl.h>
        -:   16:#include <netinet/in.h>
        -:   17:#include <errno.h>
        -:   18:#include <stdlib.h>
        -:   19:#include <stdio.h>
        -:   20:#include <string.h>
        -:   21:#include <signal.h>
        -:   22:#include <assert.h>
        -:   23:#include <pthread.h>
        -:   24:
        -:   25://#define DEBUG_SLAB_MOVER
        -:   26:/* powers-of-N allocation structures */
        -:   27:
        -:   28:typedef struct {
        -:   29:    unsigned int size;      /* sizes of items */
        -:   30:    unsigned int perslab;   /* how many items per slab */
        -:   31:
        -:   32:    void *slots;           /* list of item ptrs */
        -:   33:    unsigned int sl_curr;   /* total free items in list */
        -:   34:
        -:   35:    unsigned int slabs;     /* how many slabs were allocated for this class */
        -:   36:
        -:   37:    void **slab_list;       /* array of slab pointers */
        -:   38:    unsigned int list_size; /* size of prev array */
        -:   39:
        -:   40:    size_t requested; /* The number of requested bytes */
        -:   41:} slabclass_t;
        -:   42:
        -:   43:static slabclass_t slabclass[MAX_NUMBER_OF_SLAB_CLASSES];
        -:   44:static size_t mem_limit = 0;
        -:   45:static size_t mem_malloced = 0;
        -:   46:/* If the memory limit has been hit once. Used as a hint to decide when to
        -:   47: * early-wake the LRU maintenance thread */
        -:   48:static bool mem_limit_reached = false;
        -:   49:static int power_largest;
        -:   50:
        -:   51:static void *mem_base = NULL;
        -:   52:static void *mem_current = NULL;
        -:   53:static size_t mem_avail = 0;
        -:   54:#ifdef EXTSTORE
        -:   55:static void *storage  = NULL;
        -:   56:#endif
        -:   57:/**
        -:   58: * Access to the slab allocator is protected by this lock
        -:   59: */
        -:   60:static pthread_mutex_t slabs_lock = PTHREAD_MUTEX_INITIALIZER;
        -:   61:static pthread_mutex_t slabs_rebalance_lock = PTHREAD_MUTEX_INITIALIZER;
        -:   62:
        -:   63:/*
        -:   64: * Forward Declarations
        -:   65: */
        -:   66:static int grow_slab_list (const unsigned int id);
        -:   67:static int do_slabs_newslab(const unsigned int id);
        -:   68:static void *memory_allocate(size_t size);
        -:   69:static void do_slabs_free(void *ptr, const size_t size, unsigned int id);
        -:   70:
        -:   71:/* Preallocate as many slab pages as possible (called from slabs_init)
        -:   72:   on start-up, so users don't get confused out-of-memory errors when
        -:   73:   they do have free (in-slab) space, but no space to make new slabs.
        -:   74:   if maxslabs is 18 (POWER_LARGEST - POWER_SMALLEST + 1), then all
        -:   75:   slab types can be made.  if max memory is less than 18 MB, only the
        -:   76:   smaller ones will be made.  */
        -:   77:static void slabs_preallocate (const unsigned int maxslabs);
        -:   78:#ifdef EXTSTORE
        -:   79:void slabs_set_storage(void *arg) {
        -:   80:    storage = arg;
        -:   81:}
        -:   82:#endif
        -:   83:/*
        -:   84: * Figures out which slab class (chunk size) is required to store an item of
        -:   85: * a given size.
        -:   86: *
        -:   87: * Given object size, return id to use when allocating/freeing memory for object
        -:   88: * 0 means error: can't store such a large object
        -:   89: */
        -:   90:
   274992:   91:unsigned int slabs_clsid(const size_t size) {
   274992:   92:    int res = POWER_SMALLEST;
        -:   93:
   274992:   94:    if (size == 0 || size > settings.item_size_max)
        -:   95:        return 0;
  3771593:   96:    while (size > slabclass[res].size)
  3516242:   97:        if (res++ == power_largest)     /* won't fit in the biggest slab */
    19633:   98:            return power_largest;
   255351:   99:    return res;
        -:  100:}
        -:  101:
        -:  102:#if defined(__linux__) && defined(MADV_HUGEPAGE)
        -:  103:/* Function split out for better error path handling */
    #####:  104:static void * alloc_large_chunk_linux(const size_t limit)
        -:  105:{
    #####:  106:    size_t pagesize = 0;
    #####:  107:    void *ptr = NULL;
        -:  108:    FILE *fp;
        -:  109:    int ret;
        -:  110:
        -:  111:    /* Get the size of huge pages */
    #####:  112:    fp = fopen("/proc/meminfo", "r");
    #####:  113:    if (fp != NULL) {
        -:  114:        char buf[64];
        -:  115:
    #####:  116:        while ((fgets(buf, sizeof(buf), fp)))
    #####:  117:            if (!strncmp(buf, "Hugepagesize:", 13)) {
    #####:  118:                ret = sscanf(buf + 13, "%zu\n", &pagesize);
        -:  119:
        -:  120:                /* meminfo huge page size is in KiBs */
    #####:  121:                pagesize <<= 10;
        -:  122:            }
    #####:  123:        fclose(fp);
        -:  124:    }
        -:  125:
    #####:  126:    if (!pagesize) {
    #####:  127:        fprintf(stderr, "Failed to get supported huge page size\n");
    #####:  128:        return NULL;
        -:  129:    }
        -:  130:
    #####:  131:    if (settings.verbose > 1)
    #####:  132:        fprintf(stderr, "huge page size: %zu\n", pagesize);
        -:  133:
        -:  134:    /* This works because glibc simply uses mmap when the alignment is
        -:  135:     * above a certain limit. */
    #####:  136:    ret = posix_memalign(&ptr, pagesize, limit);
    #####:  137:    if (ret != 0) {
    #####:  138:        fprintf(stderr, "Failed to get aligned memory chunk: %d\n", ret);
    #####:  139:        return NULL;
        -:  140:    }
        -:  141:
    #####:  142:    ret = madvise(ptr, limit, MADV_HUGEPAGE);
    #####:  143:    if (ret < 0) {
    #####:  144:        fprintf(stderr, "Failed to set transparent hugepage hint: %d\n", ret);
    #####:  145:        free(ptr);
    #####:  146:        ptr = NULL;
        -:  147:    }
        -:  148:
        -:  149:    return ptr;
        -:  150:}
        -:  151:#endif
        -:  152:
        -:  153:/**
        -:  154: * Determines the chunk sizes and initializes the slab class descriptors
        -:  155: * accordingly.
        -:  156: */
       95:  157:void slabs_init(const size_t limit, const double factor, const bool prealloc, const uint32_t *slab_sizes) {
       95:  158:    int i = POWER_SMALLEST - 1;
       95:  159:    unsigned int size = sizeof(item) + settings.chunk_size;
        -:  160:
        -:  161:    /* Some platforms use runtime transparent hugepages. If for any reason
        -:  162:     * the initial allocation fails, the required settings do not persist
        -:  163:     * for remaining allocations. As such it makes little sense to do slab
        -:  164:     * preallocation. */
       95:  165:    bool __attribute__ ((unused)) do_slab_prealloc = false;
        -:  166:
       95:  167:    mem_limit = limit;
        -:  168:
       95:  169:    if (prealloc) {
        -:  170:#if defined(__linux__) && defined(MADV_HUGEPAGE)
    #####:  171:        mem_base = alloc_large_chunk_linux(mem_limit);
    #####:  172:        if (mem_base)
    #####:  173:            do_slab_prealloc = true;
        -:  174:#else
        -:  175:        /* Allocate everything in a big chunk with malloc */
        -:  176:        mem_base = malloc(mem_limit);
        -:  177:        do_slab_prealloc = true;
        -:  178:#endif
    #####:  179:        if (mem_base != NULL) {
    #####:  180:            mem_current = mem_base;
    #####:  181:            mem_avail = mem_limit;
        -:  182:        } else {
    #####:  183:            fprintf(stderr, "Warning: Failed to allocate requested memory in"
        -:  184:                    " one large chunk.\nWill allocate in smaller chunks\n");
        -:  185:        }
        -:  186:    }
        -:  187:
        -:  188:    memset(slabclass, 0, sizeof(slabclass));
        -:  189:
     3619:  190:    while (++i < MAX_NUMBER_OF_SLAB_CLASSES-1) {
     3619:  191:        if (slab_sizes != NULL) {
    #####:  192:            if (slab_sizes[i-1] == 0)
        -:  193:                break;
        -:  194:            size = slab_sizes[i-1];
     3619:  195:        } else if (size >= settings.slab_chunk_size_max / factor) {
        -:  196:            break;
        -:  197:        }
        -:  198:        /* Make sure items are always n-byte aligned */
     3524:  199:        if (size % CHUNK_ALIGN_BYTES)
     2317:  200:            size += CHUNK_ALIGN_BYTES - (size % CHUNK_ALIGN_BYTES);
        -:  201:
     3524:  202:        slabclass[i].size = size;
     3524:  203:        slabclass[i].perslab = settings.slab_page_size / slabclass[i].size;
     3524:  204:        if (slab_sizes == NULL)
     3524:  205:            size *= factor;
     3524:  206:        if (settings.verbose > 1) {
      114:  207:            fprintf(stderr, "slab class %3d: chunk size %9u perslab %7u\n",
        -:  208:                    i, slabclass[i].size, slabclass[i].perslab);
        -:  209:        }
        -:  210:    }
        -:  211:
       95:  212:    power_largest = i;
       95:  213:    slabclass[power_largest].size = settings.slab_chunk_size_max;
       95:  214:    slabclass[power_largest].perslab = settings.slab_page_size / settings.slab_chunk_size_max;
       95:  215:    if (settings.verbose > 1) {
        3:  216:        fprintf(stderr, "slab class %3d: chunk size %9u perslab %7u\n",
        -:  217:                i, slabclass[i].size, slabclass[i].perslab);
        -:  218:    }
        -:  219:
        -:  220:    /* for the test suite:  faking of how much we've already malloc'd */
        -:  221:    {
       95:  222:        char *t_initial_malloc = getenv("T_MEMD_INITIAL_MALLOC");
       95:  223:        if (t_initial_malloc) {
        1:  224:            mem_malloced = (size_t)atol(t_initial_malloc);
        -:  225:        }
        -:  226:
        -:  227:    }
        -:  228:
       95:  229:    if (prealloc && do_slab_prealloc) {
    #####:  230:        slabs_preallocate(power_largest);
        -:  231:    }
       95:  232:}
        -:  233:
    #####:  234:void slabs_prefill_global(void) {
        -:  235:    void *ptr;
    #####:  236:    slabclass_t *p = &slabclass[0];
    #####:  237:    int len = settings.slab_page_size;
        -:  238:
    #####:  239:    while (mem_malloced < mem_limit
    #####:  240:            && (ptr = memory_allocate(len)) != NULL) {
    #####:  241:        grow_slab_list(0);
    #####:  242:        p->slab_list[p->slabs++] = ptr;
        -:  243:    }
    #####:  244:    mem_limit_reached = true;
    #####:  245:}
        -:  246:
    #####:  247:static void slabs_preallocate (const unsigned int maxslabs) {
        -:  248:    int i;
    #####:  249:    unsigned int prealloc = 0;
        -:  250:
        -:  251:    /* pre-allocate a 1MB slab in every size class so people don't get
        -:  252:       confused by non-intuitive "SERVER_ERROR out of memory"
        -:  253:       messages.  this is the most common question on the mailing
        -:  254:       list.  if you really don't want this, you can rebuild without
        -:  255:       these three lines.  */
        -:  256:
    #####:  257:    for (i = POWER_SMALLEST; i < MAX_NUMBER_OF_SLAB_CLASSES; i++) {
    #####:  258:        if (++prealloc > maxslabs)
        -:  259:            return;
    #####:  260:        if (do_slabs_newslab(i) == 0) {
    #####:  261:            fprintf(stderr, "Error while preallocating slab memory!\n"
        -:  262:                "If using -L or other prealloc options, max memory must be "
        -:  263:                "at least %d megabytes.\n", power_largest);
    #####:  264:            exit(1);
        -:  265:        }
        -:  266:    }
        -:  267:}
        -:  268:
      800:  269:static int grow_slab_list (const unsigned int id) {
      800:  270:    slabclass_t *p = &slabclass[id];
      800:  271:    if (p->slabs == p->list_size) {
      181:  272:        size_t new_size =  (p->list_size != 0) ? p->list_size * 2 : 16;
      181:  273:        void *new_list = realloc(p->slab_list, new_size * sizeof(void *));
      181:  274:        if (new_list == 0) return 0;
      181:  275:        p->list_size = new_size;
      181:  276:        p->slab_list = new_list;
        -:  277:    }
        -:  278:    return 1;
        -:  279:}
        -:  280:
        -:  281:static void split_slab_page_into_freelist(char *ptr, const unsigned int id) {
        -:  282:    slabclass_t *p = &slabclass[id];
        -:  283:    int x;
   658795:  284:    for (x = 0; x < p->perslab; x++) {
   658795:  285:        do_slabs_free(ptr, 0, id);
   658795:  286:        ptr += p->size;
        -:  287:    }
        -:  288:}
        -:  289:
        -:  290:/* Fast FIFO queue */
        -:  291:static void *get_page_from_global_pool(void) {
      716:  292:    slabclass_t *p = &slabclass[SLAB_GLOBAL_PAGE_POOL];
      716:  293:    if (p->slabs < 1) {
        -:  294:        return NULL;
        -:  295:    }
       84:  296:    char *ret = p->slab_list[p->slabs - 1];
       84:  297:    p->slabs--;
        -:  298:    return ret;
        -:  299:}
        -:  300:
    29000:  301:static int do_slabs_newslab(const unsigned int id) {
    29000:  302:    slabclass_t *p = &slabclass[id];
    29000:  303:    slabclass_t *g = &slabclass[SLAB_GLOBAL_PAGE_POOL];
    29208:  304:    int len = (settings.slab_reassign || settings.slab_chunk_size_max != settings.slab_page_size)
        -:  305:        ? settings.slab_page_size
      208:  306:        : p->size * p->perslab;
        -:  307:    char *ptr;
        -:  308:
    29000:  309:    if ((mem_limit && mem_malloced + len > mem_limit && p->slabs > 0
    28344:  310:         && g->slabs == 0)) {
    28287:  311:        mem_limit_reached = true;
        -:  312:        MEMCACHED_SLABS_SLABCLASS_ALLOCATE_FAILED(id);
    28287:  313:        return 0;
        -:  314:    }
        -:  315:
     1426:  316:    if ((grow_slab_list(id) == 0) ||
      630:  317:        (((ptr = get_page_from_global_pool()) == NULL) &&
      630:  318:        ((ptr = memory_allocate((size_t)len)) == 0))) {
        -:  319:
        -:  320:        MEMCACHED_SLABS_SLABCLASS_ALLOCATE_FAILED(id);
        -:  321:        return 0;
        -:  322:    }
        -:  323:
      713:  324:    memset(ptr, 0, (size_t)len);
      713:  325:    split_slab_page_into_freelist(ptr, id);
        -:  326:
      713:  327:    p->slab_list[p->slabs++] = ptr;
        -:  328:    MEMCACHED_SLABS_SLABCLASS_ALLOCATE(id);
        -:  329:
      713:  330:    return 1;
        -:  331:}
        -:  332:
        -:  333:/*@null@*/
   285822:  334:static void *do_slabs_alloc(const size_t size, unsigned int id, uint64_t *total_bytes,
        -:  335:        unsigned int flags) {
        -:  336:    slabclass_t *p;
   285822:  337:    void *ret = NULL;
   285822:  338:    item *it = NULL;
        -:  339:
   285822:  340:    if (id < POWER_SMALLEST || id > power_largest) {
        -:  341:        MEMCACHED_SLABS_ALLOCATE_FAILED(size, 0);
        -:  342:        return NULL;
        -:  343:    }
   285822:  344:    p = &slabclass[id];
   285822:  345:    assert(p->sl_curr == 0 || ((item *)p->slots)->slabs_clsid == 0);
   285822:  346:    if (total_bytes != NULL) {
   283628:  347:        *total_bytes = p->requested;
        -:  348:    }
        -:  349:
   285822:  350:    assert(size <= p->size);
        -:  351:    /* fail unless we have space at the end of a recently allocated page,
        -:  352:       we have something on our freelist, or we could allocate a new page */
   285822:  353:    if (p->sl_curr == 0 && flags != SLABS_ALLOC_NO_NEWPAGE) {
    29000:  354:        do_slabs_newslab(id);
        -:  355:    }
        -:  356:
   285822:  357:    if (p->sl_curr != 0) {
        -:  358:        /* return off our freelist */
   257118:  359:        it = (item *)p->slots;
   257118:  360:        p->slots = it->next;
   257118:  361:        if (it->next) it->next->prev = 0;
        -:  362:        /* Kill flag and initialize refcount here for lock safety in slab
        -:  363:         * mover's freeness detection. */
   257118:  364:        it->it_flags &= ~ITEM_SLABBED;
   257118:  365:        it->refcount = 1;
   257118:  366:        p->sl_curr--;
   257118:  367:        ret = (void *)it;
        -:  368:    } else {
        -:  369:        ret = NULL;
        -:  370:    }
        -:  371:
   285822:  372:    if (ret) {
   257118:  373:        p->requested += size;
        -:  374:        MEMCACHED_SLABS_ALLOCATE(size, id, p->size, ret);
        -:  375:    } else {
        -:  376:        MEMCACHED_SLABS_ALLOCATE_FAILED(size, id);
        -:  377:    }
        -:  378:
        -:  379:    return ret;
        -:  380:}
        -:  381:
    16245:  382:static void do_slabs_free_chunked(item *it, const size_t size) {
    16245:  383:    item_chunk *chunk = (item_chunk *) ITEM_schunk(it);
        -:  384:    slabclass_t *p;
        -:  385:
    16245:  386:    it->it_flags = ITEM_SLABBED;
    16245:  387:    it->slabs_clsid = 0;
    16245:  388:    it->prev = 0;
        -:  389:    // header object's original classid is stored in chunk.
    16245:  390:    p = &slabclass[chunk->orig_clsid];
    16245:  391:    if (chunk->next) {
    16245:  392:        chunk = chunk->next;
    16245:  393:        chunk->prev = 0;
        -:  394:    } else {
        -:  395:        // header with no attached chunk
        -:  396:        chunk = NULL;
        -:  397:    }
        -:  398:
        -:  399:    // return the header object.
        -:  400:    // TODO: This is in three places, here and in do_slabs_free().
    16245:  401:    it->prev = 0;
    16245:  402:    it->next = p->slots;
    16245:  403:    if (it->next) it->next->prev = it;
    16245:  404:    p->slots = it;
    16245:  405:    p->sl_curr++;
        -:  406:    // TODO: macro
        -:  407:#ifdef NEED_ALIGN
        -:  408:    int total = it->nkey + 1 + it->nsuffix + sizeof(item) + sizeof(item_chunk);
        -:  409:    if (total % 8 != 0) {
        -:  410:        total += 8 - (total % 8);
        -:  411:    }
        -:  412:    p->requested -= total;
        -:  413:#else
    16245:  414:    p->requested -= it->nkey + 1 + it->nsuffix + sizeof(item) + sizeof(item_chunk);
        -:  415:#endif
    16245:  416:    if (settings.use_cas) {
    16245:  417:        p->requested -= sizeof(uint64_t);
        -:  418:    }
        -:  419:
        -:  420:    item_chunk *next_chunk;
   139614:  421:    while (chunk) {
   123369:  422:        assert(chunk->it_flags == ITEM_CHUNK);
   123369:  423:        chunk->it_flags = ITEM_SLABBED;
   123369:  424:        p = &slabclass[chunk->slabs_clsid];
   123369:  425:        chunk->slabs_clsid = 0;
   123369:  426:        next_chunk = chunk->next;
        -:  427:
   123369:  428:        chunk->prev = 0;
   123369:  429:        chunk->next = p->slots;
   123369:  430:        if (chunk->next) chunk->next->prev = chunk;
   123369:  431:        p->slots = chunk;
   123369:  432:        p->sl_curr++;
   123369:  433:        p->requested -= chunk->size + sizeof(item_chunk);
        -:  434:
   123369:  435:        chunk = next_chunk;
        -:  436:    }
        -:  437:
    16245:  438:    return;
        -:  439:}
        -:  440:
        -:  441:
   738594:  442:static void do_slabs_free(void *ptr, const size_t size, unsigned int id) {
        -:  443:    slabclass_t *p;
        -:  444:    item *it;
        -:  445:
   738594:  446:    assert(id >= POWER_SMALLEST && id <= power_largest);
   738594:  447:    if (id < POWER_SMALLEST || id > power_largest)
        -:  448:        return;
        -:  449:
        -:  450:    MEMCACHED_SLABS_FREE(size, id, ptr);
   738594:  451:    p = &slabclass[id];
        -:  452:
   738594:  453:    it = (item *)ptr;
   738594:  454:    if ((it->it_flags & ITEM_CHUNKED) == 0) {
        -:  455:#ifdef EXTSTORE
        -:  456:        bool is_hdr = it->it_flags & ITEM_HDR;
        -:  457:#endif
   722349:  458:        it->it_flags = ITEM_SLABBED;
   722349:  459:        it->slabs_clsid = 0;
   722349:  460:        it->prev = 0;
   722349:  461:        it->next = p->slots;
   722349:  462:        if (it->next) it->next->prev = it;
   722349:  463:        p->slots = it;
        -:  464:
   722349:  465:        p->sl_curr++;
        -:  466:#ifdef EXTSTORE
        -:  467:        if (!is_hdr) {
        -:  468:            p->requested -= size;
        -:  469:        } else {
        -:  470:            p->requested -= (size - it->nbytes) + sizeof(item_hdr);
        -:  471:        }
        -:  472:#else
   722349:  473:        p->requested -= size;
        -:  474:#endif
        -:  475:    } else {
    16245:  476:        do_slabs_free_chunked(it, size);
        -:  477:    }
        -:  478:    return;
        -:  479:}
        -:  480:
        -:  481:/* With refactoring of the various stats code the automover won't need a
        -:  482: * custom function here.
        -:  483: */
      332:  484:void fill_slab_stats_automove(slab_stats_automove *am) {
        -:  485:    int n;
      332:  486:    pthread_mutex_lock(&slabs_lock);
    21580:  487:    for (n = 0; n < MAX_NUMBER_OF_SLAB_CLASSES; n++) {
    21248:  488:        slabclass_t *p = &slabclass[n];
    21248:  489:        slab_stats_automove *cur = &am[n];
    21248:  490:        cur->chunks_per_page = p->perslab;
    21248:  491:        cur->free_chunks = p->sl_curr;
    21248:  492:        cur->total_pages = p->slabs;
    21248:  493:        cur->chunk_size = p->size;
        -:  494:    }
      332:  495:    pthread_mutex_unlock(&slabs_lock);
      332:  496:}
        -:  497:
        -:  498:/* TODO: slabs_available_chunks should grow up to encompass this.
        -:  499: * mem_flag is redundant with the other function.
        -:  500: */
    #####:  501:unsigned int global_page_pool_size(bool *mem_flag) {
    #####:  502:    unsigned int ret = 0;
    #####:  503:    pthread_mutex_lock(&slabs_lock);
    #####:  504:    if (mem_flag != NULL)
    #####:  505:        *mem_flag = mem_malloced >= mem_limit ? true : false;
    #####:  506:    ret = slabclass[SLAB_GLOBAL_PAGE_POOL].slabs;
    #####:  507:    pthread_mutex_unlock(&slabs_lock);
    #####:  508:    return ret;
        -:  509:}
        -:  510:
       57:  511:static int nz_strcmp(int nzlength, const char *nz, const char *z) {
       57:  512:    int zlength=strlen(z);
       57:  513:    return (zlength == nzlength) && (strncmp(nz, z, zlength) == 0) ? 0 : -1;
        -:  514:}
        -:  515:
     2681:  516:bool get_stats(const char *stat_type, int nkey, ADD_STAT add_stats, void *c) {
     2681:  517:    bool ret = true;
        -:  518:
     2681:  519:    if (add_stats != NULL) {
     2681:  520:        if (!stat_type) {
        -:  521:            /* prepare general statistics for the engine */
     2647:  522:            STATS_LOCK();
     2647:  523:            APPEND_STAT("bytes", "%llu", (unsigned long long)stats_state.curr_bytes);
     2647:  524:            APPEND_STAT("curr_items", "%llu", (unsigned long long)stats_state.curr_items);
     2647:  525:            APPEND_STAT("total_items", "%llu", (unsigned long long)stats.total_items);
     2647:  526:            STATS_UNLOCK();
     2647:  527:            pthread_mutex_lock(&slabs_lock);
     2647:  528:            APPEND_STAT("slab_global_page_pool", "%u", slabclass[SLAB_GLOBAL_PAGE_POOL].slabs);
     2647:  529:            pthread_mutex_unlock(&slabs_lock);
     2647:  530:            item_stats_totals(add_stats, c);
       34:  531:        } else if (nz_strcmp(nkey, stat_type, "items") == 0) {
       11:  532:            item_stats(add_stats, c);
       23:  533:        } else if (nz_strcmp(nkey, stat_type, "slabs") == 0) {
       23:  534:            slabs_stats(add_stats, c);
    #####:  535:        } else if (nz_strcmp(nkey, stat_type, "sizes") == 0) {
    #####:  536:            item_stats_sizes(add_stats, c);
    #####:  537:        } else if (nz_strcmp(nkey, stat_type, "sizes_enable") == 0) {
    #####:  538:            item_stats_sizes_enable(add_stats, c);
    #####:  539:        } else if (nz_strcmp(nkey, stat_type, "sizes_disable") == 0) {
    #####:  540:            item_stats_sizes_disable(add_stats, c);
        -:  541:        } else {
        -:  542:            ret = false;
        -:  543:        }
        -:  544:    } else {
        -:  545:        ret = false;
        -:  546:    }
        -:  547:
     2681:  548:    return ret;
        -:  549:}
        -:  550:
        -:  551:/*@null@*/
       23:  552:static void do_slabs_stats(ADD_STAT add_stats, void *c) {
        -:  553:    int i, total;
        -:  554:    /* Get the per-thread stats which contain some interesting aggregates */
        -:  555:    struct thread_stats thread_stats;
       23:  556:    threadlocal_stats_aggregate(&thread_stats);
        -:  557:
       23:  558:    total = 0;
      910:  559:    for(i = POWER_SMALLEST; i <= power_largest; i++) {
      887:  560:        slabclass_t *p = &slabclass[i];
      887:  561:        if (p->slabs != 0) {
        -:  562:            uint32_t perslab, slabs;
       69:  563:            slabs = p->slabs;
       69:  564:            perslab = p->perslab;
        -:  565:
        -:  566:            char key_str[STAT_KEY_LEN];
        -:  567:            char val_str[STAT_VAL_LEN];
       69:  568:            int klen = 0, vlen = 0;
        -:  569:
      138:  570:            APPEND_NUM_STAT(i, "chunk_size", "%u", p->size);
      138:  571:            APPEND_NUM_STAT(i, "chunks_per_page", "%u", perslab);
      138:  572:            APPEND_NUM_STAT(i, "total_pages", "%u", slabs);
      138:  573:            APPEND_NUM_STAT(i, "total_chunks", "%u", slabs * perslab);
      138:  574:            APPEND_NUM_STAT(i, "used_chunks", "%u",
        -:  575:                            slabs*perslab - p->sl_curr);
      138:  576:            APPEND_NUM_STAT(i, "free_chunks", "%u", p->sl_curr);
        -:  577:            /* Stat is dead, but displaying zero instead of removing it. */
      138:  578:            APPEND_NUM_STAT(i, "free_chunks_end", "%u", 0);
      138:  579:            APPEND_NUM_STAT(i, "mem_requested", "%llu",
        -:  580:                            (unsigned long long)p->requested);
      138:  581:            APPEND_NUM_STAT(i, "get_hits", "%llu",
        -:  582:                    (unsigned long long)thread_stats.slab_stats[i].get_hits);
      138:  583:            APPEND_NUM_STAT(i, "cmd_set", "%llu",
        -:  584:                    (unsigned long long)thread_stats.slab_stats[i].set_cmds);
      138:  585:            APPEND_NUM_STAT(i, "delete_hits", "%llu",
        -:  586:                    (unsigned long long)thread_stats.slab_stats[i].delete_hits);
      138:  587:            APPEND_NUM_STAT(i, "incr_hits", "%llu",
        -:  588:                    (unsigned long long)thread_stats.slab_stats[i].incr_hits);
      138:  589:            APPEND_NUM_STAT(i, "decr_hits", "%llu",
        -:  590:                    (unsigned long long)thread_stats.slab_stats[i].decr_hits);
      138:  591:            APPEND_NUM_STAT(i, "cas_hits", "%llu",
        -:  592:                    (unsigned long long)thread_stats.slab_stats[i].cas_hits);
      138:  593:            APPEND_NUM_STAT(i, "cas_badval", "%llu",
        -:  594:                    (unsigned long long)thread_stats.slab_stats[i].cas_badval);
      138:  595:            APPEND_NUM_STAT(i, "touch_hits", "%llu",
        -:  596:                    (unsigned long long)thread_stats.slab_stats[i].touch_hits);
       69:  597:            total++;
        -:  598:        }
        -:  599:    }
        -:  600:
        -:  601:    /* add overall slab stats and append terminator */
        -:  602:
       23:  603:    APPEND_STAT("active_slabs", "%d", total);
       23:  604:    APPEND_STAT("total_malloced", "%llu", (unsigned long long)mem_malloced);
       23:  605:    add_stats(NULL, 0, NULL, 0, c);
       23:  606:}
        -:  607:
      630:  608:static void *memory_allocate(size_t size) {
        -:  609:    void *ret;
        -:  610:
      630:  611:    if (mem_base == NULL) {
        -:  612:        /* We are not using a preallocated large memory chunk */
      630:  613:        ret = malloc(size);
        -:  614:    } else {
    #####:  615:        ret = mem_current;
        -:  616:
    #####:  617:        if (size > mem_avail) {
        -:  618:            return NULL;
        -:  619:        }
        -:  620:
        -:  621:        /* mem_current pointer _must_ be aligned!!! */
    #####:  622:        if (size % CHUNK_ALIGN_BYTES) {
    #####:  623:            size += CHUNK_ALIGN_BYTES - (size % CHUNK_ALIGN_BYTES);
        -:  624:        }
        -:  625:
    #####:  626:        mem_current = ((char*)mem_current) + size;
    #####:  627:        if (size < mem_avail) {
    #####:  628:            mem_avail -= size;
        -:  629:        } else {
    #####:  630:            mem_avail = 0;
        -:  631:        }
        -:  632:    }
      630:  633:    mem_malloced += size;
        -:  634:
      630:  635:    return ret;
        -:  636:}
        -:  637:
        -:  638:/* Must only be used if all pages are item_size_max */
       87:  639:static void memory_release() {
       87:  640:    void *p = NULL;
       87:  641:    if (mem_base != NULL)
        -:  642:        return;
        -:  643:
       87:  644:    if (!settings.slab_reassign)
        -:  645:        return;
        -:  646:
       91:  647:    while (mem_malloced > mem_limit &&
        -:  648:            (p = get_page_from_global_pool()) != NULL) {
        1:  649:        free(p);
        1:  650:        mem_malloced -= settings.slab_page_size;
        -:  651:    }
        -:  652:}
        -:  653:
   283628:  654:void *slabs_alloc(size_t size, unsigned int id, uint64_t *total_bytes,
        -:  655:        unsigned int flags) {
        -:  656:    void *ret;
        -:  657:
   283628:  658:    pthread_mutex_lock(&slabs_lock);
   283628:  659:    ret = do_slabs_alloc(size, id, total_bytes, flags);
   283628:  660:    pthread_mutex_unlock(&slabs_lock);
   283628:  661:    return ret;
        -:  662:}
        -:  663:
    79799:  664:void slabs_free(void *ptr, size_t size, unsigned int id) {
    79799:  665:    pthread_mutex_lock(&slabs_lock);
    79799:  666:    do_slabs_free(ptr, size, id);
    79799:  667:    pthread_mutex_unlock(&slabs_lock);
    79799:  668:}
        -:  669:
       23:  670:void slabs_stats(ADD_STAT add_stats, void *c) {
       23:  671:    pthread_mutex_lock(&slabs_lock);
       23:  672:    do_slabs_stats(add_stats, c);
       23:  673:    pthread_mutex_unlock(&slabs_lock);
       23:  674:}
        -:  675:
        -:  676:static bool do_slabs_adjust_mem_limit(size_t new_mem_limit) {
        -:  677:    /* Cannot adjust memory limit at runtime if prealloc'ed */
        3:  678:    if (mem_base != NULL)
        -:  679:        return false;
        3:  680:    settings.maxbytes = new_mem_limit;
        3:  681:    mem_limit = new_mem_limit;
        3:  682:    mem_limit_reached = false; /* Will reset on next alloc */
        3:  683:    memory_release(); /* free what might already be in the global pool */
        -:  684:    return true;
        -:  685:}
        -:  686:
        3:  687:bool slabs_adjust_mem_limit(size_t new_mem_limit) {
        -:  688:    bool ret;
        3:  689:    pthread_mutex_lock(&slabs_lock);
        3:  690:    ret = do_slabs_adjust_mem_limit(new_mem_limit);
        3:  691:    pthread_mutex_unlock(&slabs_lock);
        3:  692:    return ret;
        -:  693:}
        -:  694:
    #####:  695:void slabs_adjust_mem_requested(unsigned int id, size_t old, size_t ntotal)
        -:  696:{
    #####:  697:    pthread_mutex_lock(&slabs_lock);
        -:  698:    slabclass_t *p;
    #####:  699:    if (id < POWER_SMALLEST || id > power_largest) {
    #####:  700:        fprintf(stderr, "Internal error! Invalid slab class\n");
    #####:  701:        abort();
        -:  702:    }
        -:  703:
    #####:  704:    p = &slabclass[id];
    #####:  705:    p->requested = p->requested - old + ntotal;
    #####:  706:    pthread_mutex_unlock(&slabs_lock);
    #####:  707:}
        -:  708:
   222068:  709:unsigned int slabs_available_chunks(const unsigned int id, bool *mem_flag,
        -:  710:        uint64_t *total_bytes, unsigned int *chunks_perslab) {
        -:  711:    unsigned int ret;
        -:  712:    slabclass_t *p;
        -:  713:
   222068:  714:    pthread_mutex_lock(&slabs_lock);
   222068:  715:    p = &slabclass[id];
   222068:  716:    ret = p->sl_curr;
   222068:  717:    if (mem_flag != NULL)
    #####:  718:        *mem_flag = mem_malloced >= mem_limit ? true : false;
   222068:  719:    if (total_bytes != NULL)
   222068:  720:        *total_bytes = p->requested;
   222068:  721:    if (chunks_perslab != NULL)
   222068:  722:        *chunks_perslab = p->perslab;
   222068:  723:    pthread_mutex_unlock(&slabs_lock);
   222068:  724:    return ret;
        -:  725:}
        -:  726:
        -:  727:/* The slabber system could avoid needing to understand much, if anything,
        -:  728: * about items if callbacks were strategically used. Due to how the slab mover
        -:  729: * works, certain flag bits can only be adjusted while holding the slabs lock.
        -:  730: * Using these functions, isolate sections of code needing this and turn them
        -:  731: * into callbacks when an interface becomes more obvious.
        -:  732: */
   136171:  733:void slabs_mlock(void) {
   136171:  734:    pthread_mutex_lock(&slabs_lock);
   136171:  735:}
        -:  736:
   136171:  737:void slabs_munlock(void) {
   136171:  738:    pthread_mutex_unlock(&slabs_lock);
   136171:  739:}
        -:  740:
        -:  741:static pthread_cond_t slab_rebalance_cond = PTHREAD_COND_INITIALIZER;
        -:  742:static volatile int do_run_slab_thread = 1;
        -:  743:static volatile int do_run_slab_rebalance_thread = 1;
        -:  744:
        -:  745:#define DEFAULT_SLAB_BULK_CHECK 1
        -:  746:int slab_bulk_check = DEFAULT_SLAB_BULK_CHECK;
        -:  747:
       87:  748:static int slab_rebalance_start(void) {
        -:  749:    slabclass_t *s_cls;
       87:  750:    int no_go = 0;
        -:  751:
       87:  752:    pthread_mutex_lock(&slabs_lock);
        -:  753:
      174:  754:    if (slab_rebal.s_clsid < SLAB_GLOBAL_PAGE_POOL ||
      174:  755:        slab_rebal.s_clsid > power_largest  ||
      174:  756:        slab_rebal.d_clsid < SLAB_GLOBAL_PAGE_POOL ||
       87:  757:        slab_rebal.d_clsid > power_largest  ||
        -:  758:        slab_rebal.s_clsid == slab_rebal.d_clsid)
    #####:  759:        no_go = -2;
        -:  760:
       87:  761:    s_cls = &slabclass[slab_rebal.s_clsid];
        -:  762:
       87:  763:    if (!grow_slab_list(slab_rebal.d_clsid)) {
    #####:  764:        no_go = -1;
        -:  765:    }
        -:  766:
       87:  767:    if (s_cls->slabs < 2)
    #####:  768:        no_go = -3;
        -:  769:
       87:  770:    if (no_go != 0) {
    #####:  771:        pthread_mutex_unlock(&slabs_lock);
    #####:  772:        return no_go; /* Should use a wrapper function... */
        -:  773:    }
        -:  774:
        -:  775:    /* Always kill the first available slab page as it is most likely to
        -:  776:     * contain the oldest items
        -:  777:     */
       87:  778:    slab_rebal.slab_start = s_cls->slab_list[0];
       87:  779:    slab_rebal.slab_end   = (char *)slab_rebal.slab_start +
       87:  780:        (s_cls->size * s_cls->perslab);
       87:  781:    slab_rebal.slab_pos   = slab_rebal.slab_start;
       87:  782:    slab_rebal.done       = 0;
        -:  783:    // Don't need to do chunk move work if page is in global pool.
       87:  784:    if (slab_rebal.s_clsid == SLAB_GLOBAL_PAGE_POOL) {
    #####:  785:        slab_rebal.done = 1;
        -:  786:    }
        -:  787:
       87:  788:    slab_rebalance_signal = 2;
        -:  789:
       87:  790:    if (settings.verbose > 1) {
    #####:  791:        fprintf(stderr, "Started a slab rebalance\n");
        -:  792:    }
        -:  793:
       87:  794:    pthread_mutex_unlock(&slabs_lock);
        -:  795:
       87:  796:    STATS_LOCK();
       87:  797:    stats_state.slab_reassign_running = true;
       87:  798:    STATS_UNLOCK();
        -:  799:
       87:  800:    return 0;
        -:  801:}
        -:  802:
        -:  803:/* CALLED WITH slabs_lock HELD */
     1781:  804:static void *slab_rebalance_alloc(const size_t size, unsigned int id) {
        -:  805:    slabclass_t *s_cls;
     1781:  806:    s_cls = &slabclass[slab_rebal.s_clsid];
        -:  807:    int x;
     1781:  808:    item *new_it = NULL;
        -:  809:
     2194:  810:    for (x = 0; x < s_cls->perslab; x++) {
     2194:  811:        new_it = do_slabs_alloc(size, id, NULL, SLABS_ALLOC_NO_NEWPAGE);
        -:  812:        /* check that memory isn't within the range to clear */
     2194:  813:        if (new_it == NULL) {
        -:  814:            break;
        -:  815:        }
     1777:  816:        if ((void *)new_it >= slab_rebal.slab_start
      413:  817:            && (void *)new_it < slab_rebal.slab_end) {
        -:  818:            /* Pulled something we intend to free. Mark it as freed since
        -:  819:             * we've already done the work of unlinking it from the freelist.
        -:  820:             */
      413:  821:            s_cls->requested -= size;
      413:  822:            new_it->refcount = 0;
      413:  823:            new_it->it_flags = ITEM_SLABBED|ITEM_FETCHED;
        -:  824:#ifdef DEBUG_SLAB_MOVER
        -:  825:            memcpy(ITEM_key(new_it), "deadbeef", 8);
        -:  826:#endif
      413:  827:            new_it = NULL;
      413:  828:            slab_rebal.inline_reclaim++;
        -:  829:        } else {
        -:  830:            break;
        -:  831:        }
        -:  832:    }
     1781:  833:    return new_it;
        -:  834:}
        -:  835:
        -:  836:/* CALLED WITH slabs_lock HELD */
        -:  837:/* detaches item/chunk from freelist. */
     8166:  838:static void slab_rebalance_cut_free(slabclass_t *s_cls, item *it) {
        -:  839:    /* Ensure this was on the freelist and nothing else. */
     8166:  840:    assert(it->it_flags == ITEM_SLABBED);
     8166:  841:    if (s_cls->slots == it) {
       30:  842:        s_cls->slots = it->next;
        -:  843:    }
     8166:  844:    if (it->next) it->next->prev = it->prev;
     8166:  845:    if (it->prev) it->prev->next = it->next;
     8166:  846:    s_cls->sl_curr--;
     8166:  847:}
        -:  848:
        -:  849:enum move_status {
        -:  850:    MOVE_PASS=0, MOVE_FROM_SLAB, MOVE_FROM_LRU, MOVE_BUSY, MOVE_LOCKED
        -:  851:};
        -:  852:
        -:  853:#define SLAB_MOVE_MAX_LOOPS 1000
        -:  854:
        -:  855:/* refcount == 0 is safe since nobody can incr while item_lock is held.
        -:  856: * refcount != 0 is impossible since flags/etc can be modified in other
        -:  857: * threads. instead, note we found a busy one and bail. logic in do_item_get
        -:  858: * will prevent busy items from continuing to be busy
        -:  859: * NOTE: This is checking it_flags outside of an item lock. I believe this
        -:  860: * works since it_flags is 8 bits, and we're only ever comparing a single bit
        -:  861: * regardless. ITEM_SLABBED bit will always be correct since we're holding the
        -:  862: * lock which modifies that bit. ITEM_LINKED won't exist if we're between an
        -:  863: * item having ITEM_SLABBED removed, and the key hasn't been added to the item
        -:  864: * yet. The memory barrier from the slabs lock should order the key write and the
        -:  865: * flags to the item?
        -:  866: * If ITEM_LINKED did exist and was just removed, but we still see it, that's
        -:  867: * still safe since it will have a valid key, which we then lock, and then
        -:  868: * recheck everything.
        -:  869: * This may not be safe on all platforms; If not, slabs_alloc() will need to
        -:  870: * seed the item key while holding slabs_lock.
        -:  871: */
    10872:  872:static int slab_rebalance_move(void) {
        -:  873:    slabclass_t *s_cls;
        -:  874:    int x;
    10872:  875:    int was_busy = 0;
    10872:  876:    int refcount = 0;
        -:  877:    uint32_t hv;
        -:  878:    void *hold_lock;
    10872:  879:    enum move_status status = MOVE_PASS;
        -:  880:
    10872:  881:    pthread_mutex_lock(&slabs_lock);
        -:  882:
    10872:  883:    s_cls = &slabclass[slab_rebal.s_clsid];
        -:  884:
    21653:  885:    for (x = 0; x < slab_bulk_check; x++) {
    10872:  886:        hv = 0;
    10872:  887:        hold_lock = NULL;
    10872:  888:        item *it = slab_rebal.slab_pos;
    10872:  889:        item_chunk *ch = NULL;
    10872:  890:        status = MOVE_PASS;
    10872:  891:        if (it->it_flags & ITEM_CHUNK) {
        -:  892:            /* This chunk is a chained part of a larger item. */
      445:  893:            ch = (item_chunk *) it;
        -:  894:            /* Instead, we use the head chunk to find the item and effectively
        -:  895:             * lock the entire structure. If a chunk has ITEM_CHUNK flag, its
        -:  896:             * head cannot be slabbed, so the normal routine is safe. */
      445:  897:            it = ch->head;
      445:  898:            assert(it->it_flags & ITEM_CHUNKED);
        -:  899:        }
        -:  900:
        -:  901:        /* ITEM_FETCHED when ITEM_SLABBED is overloaded to mean we've cleared
        -:  902:         * the chunk for move. Only these two flags should exist.
        -:  903:         */
    10872:  904:        if (it->it_flags != (ITEM_SLABBED|ITEM_FETCHED)) {
        -:  905:            /* ITEM_SLABBED can only be added/removed under the slabs_lock */
     9947:  906:            if (it->it_flags & ITEM_SLABBED) {
     8166:  907:                assert(ch == NULL);
     8166:  908:                slab_rebalance_cut_free(s_cls, it);
     8166:  909:                status = MOVE_FROM_SLAB;
     1781:  910:            } else if ((it->it_flags & ITEM_LINKED) != 0) {
        -:  911:                /* If it doesn't have ITEM_SLABBED, the item could be in any
        -:  912:                 * state on its way to being freed or written to. If no
        -:  913:                 * ITEM_SLABBED, but it's had ITEM_LINKED, it must be active
        -:  914:                 * and have the key written to it already.
        -:  915:                 */
     1781:  916:                hv = hash(ITEM_key(it), it->nkey);
     1781:  917:                if ((hold_lock = item_trylock(hv)) == NULL) {
        -:  918:                    status = MOVE_LOCKED;
        -:  919:                } else {
     1781:  920:                    bool is_linked = (it->it_flags & ITEM_LINKED);
     1781:  921:                    refcount = refcount_incr(it);
     1781:  922:                    if (refcount == 2) { /* item is linked but not busy */
        -:  923:                        /* Double check ITEM_LINKED flag here, since we're
        -:  924:                         * past a memory barrier from the mutex. */
     1781:  925:                        if (is_linked) {
        -:  926:                            status = MOVE_FROM_LRU;
        -:  927:                        } else {
        -:  928:                            /* refcount == 1 + !ITEM_LINKED means the item is being
        -:  929:                             * uploaded to, or was just unlinked but hasn't been freed
        -:  930:                             * yet. Let it bleed off on its own and try again later */
    #####:  931:                            status = MOVE_BUSY;
        -:  932:                        }
    #####:  933:                    } else if (refcount > 2 && is_linked) {
        -:  934:                        // TODO: Mark items for delete/rescue and process
        -:  935:                        // outside of the main loop.
    #####:  936:                        if (slab_rebal.busy_loops > SLAB_MOVE_MAX_LOOPS) {
    #####:  937:                            slab_rebal.busy_deletes++;
        -:  938:                            // Only safe to hold slabs lock because refcount
        -:  939:                            // can't drop to 0 until we release item lock.
        -:  940:                            STORAGE_delete(storage, it);
    #####:  941:                            pthread_mutex_unlock(&slabs_lock);
    #####:  942:                            do_item_unlink(it, hv);
    #####:  943:                            pthread_mutex_lock(&slabs_lock);
        -:  944:                        }
        -:  945:                        status = MOVE_BUSY;
        -:  946:                    } else {
    #####:  947:                        if (settings.verbose > 2) {
    #####:  948:                            fprintf(stderr, "Slab reassign hit a busy item: refcount: %d (%d -> %d)\n",
        -:  949:                                it->refcount, slab_rebal.s_clsid, slab_rebal.d_clsid);
        -:  950:                        }
        -:  951:                        status = MOVE_BUSY;
        -:  952:                    }
        -:  953:                    /* Item lock must be held while modifying refcount */
     1781:  954:                    if (status == MOVE_BUSY) {
    #####:  955:                        refcount_decr(it);
    #####:  956:                        item_trylock_unlock(hold_lock);
        -:  957:                    }
        -:  958:                }
        -:  959:            } else {
        -:  960:                /* See above comment. No ITEM_SLABBED or ITEM_LINKED. Mark
        -:  961:                 * busy and wait for item to complete its upload. */
        -:  962:                status = MOVE_BUSY;
        -:  963:            }
        -:  964:        }
        -:  965:
    10872:  966:        int save_item = 0;
    10872:  967:        item *new_it = NULL;
    10872:  968:        size_t ntotal = 0;
    10872:  969:        switch (status) {
        -:  970:            case MOVE_FROM_LRU:
        -:  971:                /* Lock order is LRU locks -> slabs_lock. unlink uses LRU lock.
        -:  972:                 * We only need to hold the slabs_lock while initially looking
        -:  973:                 * at an item, and at this point we have an exclusive refcount
        -:  974:                 * (2) + the item is locked. Drop slabs lock, drop item to
        -:  975:                 * refcount 1 (just our own, then fall through and wipe it
        -:  976:                 */
        -:  977:                /* Check if expired or flushed */
     1781:  978:                ntotal = ITEM_ntotal(it);
        -:  979:#ifdef EXTSTORE
        -:  980:                if (it->it_flags & ITEM_HDR) {
        -:  981:                    ntotal = (ntotal - it->nbytes) + sizeof(item_hdr);
        -:  982:                }
        -:  983:#endif
        -:  984:                /* REQUIRES slabs_lock: CHECK FOR cls->sl_curr > 0 */
     1781:  985:                if (ch == NULL && (it->it_flags & ITEM_CHUNKED)) {
        -:  986:                    /* Chunked should be identical to non-chunked, except we need
        -:  987:                     * to swap out ntotal for the head-chunk-total. */
    #####:  988:                    ntotal = s_cls->size;
        -:  989:                }
     1781:  990:                if ((it->exptime != 0 && it->exptime < current_time)
     1781:  991:                    || item_is_flushed(it)) {
        -:  992:                    /* Expired, don't save. */
        -:  993:                    save_item = 0;
     3117:  994:                } else if (ch == NULL &&
     1336:  995:                        (new_it = slab_rebalance_alloc(ntotal, slab_rebal.s_clsid)) == NULL) {
        -:  996:                    /* Not a chunk of an item, and nomem. */
      161:  997:                    save_item = 0;
      161:  998:                    slab_rebal.evictions_nomem++;
     2065:  999:                } else if (ch != NULL &&
      445: 1000:                        (new_it = slab_rebalance_alloc(s_cls->size, slab_rebal.s_clsid)) == NULL) {
        -: 1001:                    /* Is a chunk of an item, and nomem. */
      256: 1002:                    save_item = 0;
      256: 1003:                    slab_rebal.evictions_nomem++;
        -: 1004:                } else {
        -: 1005:                    /* Was whatever it was, and we have memory for it. */
        -: 1006:                    save_item = 1;
        -: 1007:                }
     1781: 1008:                pthread_mutex_unlock(&slabs_lock);
     1781: 1009:                unsigned int requested_adjust = 0;
     1781: 1010:                if (save_item) {
     1364: 1011:                    if (ch == NULL) {
     1175: 1012:                        assert((new_it->it_flags & ITEM_CHUNKED) == 0);
        -: 1013:                        /* if free memory, memcpy. clear prev/next/h_bucket */
     1175: 1014:                        memcpy(new_it, it, ntotal);
     1175: 1015:                        new_it->prev = 0;
     1175: 1016:                        new_it->next = 0;
     1175: 1017:                        new_it->h_next = 0;
        -: 1018:                        /* These are definitely required. else fails assert */
     1175: 1019:                        new_it->it_flags &= ~ITEM_LINKED;
     1175: 1020:                        new_it->refcount = 0;
     1175: 1021:                        do_item_replace(it, new_it, hv);
        -: 1022:                        /* Need to walk the chunks and repoint head  */
     1175: 1023:                        if (new_it->it_flags & ITEM_CHUNKED) {
    #####: 1024:                            item_chunk *fch = (item_chunk *) ITEM_schunk(new_it);
    #####: 1025:                            fch->next->prev = fch;
    #####: 1026:                            while (fch) {
    #####: 1027:                                fch->head = new_it;
    #####: 1028:                                fch = fch->next;
        -: 1029:                            }
        -: 1030:                        }
     1175: 1031:                        it->refcount = 0;
     1175: 1032:                        it->it_flags = ITEM_SLABBED|ITEM_FETCHED;
        -: 1033:#ifdef DEBUG_SLAB_MOVER
        -: 1034:                        memcpy(ITEM_key(it), "deadbeef", 8);
        -: 1035:#endif
     1175: 1036:                        slab_rebal.rescues++;
     1175: 1037:                        requested_adjust = ntotal;
        -: 1038:                    } else {
      189: 1039:                        item_chunk *nch = (item_chunk *) new_it;
        -: 1040:                        /* Chunks always have head chunk (the main it) */
      189: 1041:                        ch->prev->next = nch;
      189: 1042:                        if (ch->next)
       19: 1043:                            ch->next->prev = nch;
      378: 1044:                        memcpy(nch, ch, ch->used + sizeof(item_chunk));
      189: 1045:                        ch->refcount = 0;
      189: 1046:                        ch->it_flags = ITEM_SLABBED|ITEM_FETCHED;
      189: 1047:                        slab_rebal.chunk_rescues++;
        -: 1048:#ifdef DEBUG_SLAB_MOVER
        -: 1049:                        memcpy(ITEM_key((item *)ch), "deadbeef", 8);
        -: 1050:#endif
      189: 1051:                        refcount_decr(it);
      189: 1052:                        requested_adjust = s_cls->size;
        -: 1053:                    }
        -: 1054:                } else {
        -: 1055:                    /* restore ntotal in case we tried saving a head chunk. */
      417: 1056:                    ntotal = ITEM_ntotal(it);
        -: 1057:                    STORAGE_delete(storage, it);
      417: 1058:                    do_item_unlink(it, hv);
      417: 1059:                    slabs_free(it, ntotal, slab_rebal.s_clsid);
        -: 1060:                    /* Swing around again later to remove it from the freelist. */
      417: 1061:                    slab_rebal.busy_items++;
      417: 1062:                    was_busy++;
        -: 1063:                }
     1781: 1064:                item_trylock_unlock(hold_lock);
     1781: 1065:                pthread_mutex_lock(&slabs_lock);
        -: 1066:                /* Always remove the ntotal, as we added it in during
        -: 1067:                 * do_slabs_alloc() when copying the item.
        -: 1068:                 */
     1781: 1069:                s_cls->requested -= requested_adjust;
     1781: 1070:                break;
        -: 1071:            case MOVE_FROM_SLAB:
     8166: 1072:                it->refcount = 0;
     8166: 1073:                it->it_flags = ITEM_SLABBED|ITEM_FETCHED;
        -: 1074:#ifdef DEBUG_SLAB_MOVER
        -: 1075:                memcpy(ITEM_key(it), "deadbeef", 8);
        -: 1076:#endif
     8166: 1077:                break;
        -: 1078:            case MOVE_BUSY:
        -: 1079:            case MOVE_LOCKED:
    #####: 1080:                slab_rebal.busy_items++;
    #####: 1081:                was_busy++;
    #####: 1082:                break;
        -: 1083:            case MOVE_PASS:
        -: 1084:                break;
        -: 1085:        }
        -: 1086:
    10872: 1087:        slab_rebal.slab_pos = (char *)slab_rebal.slab_pos + s_cls->size;
    10872: 1088:        if (slab_rebal.slab_pos >= slab_rebal.slab_end)
        -: 1089:            break;
        -: 1090:    }
        -: 1091:
    10872: 1092:    if (slab_rebal.slab_pos >= slab_rebal.slab_end) {
        -: 1093:        /* Some items were busy, start again from the top */
       91: 1094:        if (slab_rebal.busy_items) {
        5: 1095:            slab_rebal.slab_pos = slab_rebal.slab_start;
        5: 1096:            STATS_LOCK();
        5: 1097:            stats.slab_reassign_busy_items += slab_rebal.busy_items;
        5: 1098:            STATS_UNLOCK();
        5: 1099:            slab_rebal.busy_items = 0;
        5: 1100:            slab_rebal.busy_loops++;
        -: 1101:        } else {
       86: 1102:            slab_rebal.done++;
        -: 1103:        }
        -: 1104:    }
        -: 1105:
    10872: 1106:    pthread_mutex_unlock(&slabs_lock);
        -: 1107:
    10872: 1108:    return was_busy;
        -: 1109:}
        -: 1110:
       86: 1111:static void slab_rebalance_finish(void) {
        -: 1112:    slabclass_t *s_cls;
        -: 1113:    slabclass_t *d_cls;
        -: 1114:    int x;
        -: 1115:    uint32_t rescues;
        -: 1116:    uint32_t evictions_nomem;
        -: 1117:    uint32_t inline_reclaim;
        -: 1118:    uint32_t chunk_rescues;
        -: 1119:    uint32_t busy_deletes;
        -: 1120:
       86: 1121:    pthread_mutex_lock(&slabs_lock);
        -: 1122:
       86: 1123:    s_cls = &slabclass[slab_rebal.s_clsid];
       86: 1124:    d_cls = &slabclass[slab_rebal.d_clsid];
        -: 1125:
        -: 1126:#ifdef DEBUG_SLAB_MOVER
        -: 1127:    /* If the algorithm is broken, live items can sneak in. */
        -: 1128:    slab_rebal.slab_pos = slab_rebal.slab_start;
        -: 1129:    while (1) {
        -: 1130:        item *it = slab_rebal.slab_pos;
        -: 1131:        assert(it->it_flags == (ITEM_SLABBED|ITEM_FETCHED));
        -: 1132:        assert(memcmp(ITEM_key(it), "deadbeef", 8) == 0);
        -: 1133:        it->it_flags = ITEM_SLABBED|ITEM_FETCHED;
        -: 1134:        slab_rebal.slab_pos = (char *)slab_rebal.slab_pos + s_cls->size;
        -: 1135:        if (slab_rebal.slab_pos >= slab_rebal.slab_end)
        -: 1136:            break;
        -: 1137:    }
        -: 1138:#endif
        -: 1139:
        -: 1140:    /* At this point the stolen slab is completely clear.
        -: 1141:     * We always kill the "first"/"oldest" slab page in the slab_list, so
        -: 1142:     * shuffle the page list backwards and decrement.
        -: 1143:     */
       86: 1144:    s_cls->slabs--;
     2166: 1145:    for (x = 0; x < s_cls->slabs; x++) {
     2080: 1146:        s_cls->slab_list[x] = s_cls->slab_list[x+1];
        -: 1147:    }
        -: 1148:
       86: 1149:    d_cls->slab_list[d_cls->slabs++] = slab_rebal.slab_start;
        -: 1150:    /* Don't need to split the page into chunks if we're just storing it */
       86: 1151:    if (slab_rebal.d_clsid > SLAB_GLOBAL_PAGE_POOL) {
        4: 1152:        memset(slab_rebal.slab_start, 0, (size_t)settings.slab_page_size);
        2: 1153:        split_slab_page_into_freelist(slab_rebal.slab_start,
        2: 1154:            slab_rebal.d_clsid);
       84: 1155:    } else if (slab_rebal.d_clsid == SLAB_GLOBAL_PAGE_POOL) {
        -: 1156:        /* mem_malloc'ed might be higher than mem_limit. */
       84: 1157:        mem_limit_reached = false;
       84: 1158:        memory_release();
        -: 1159:    }
        -: 1160:
       86: 1161:    slab_rebal.busy_loops = 0;
       86: 1162:    slab_rebal.done       = 0;
       86: 1163:    slab_rebal.s_clsid    = 0;
       86: 1164:    slab_rebal.d_clsid    = 0;
       86: 1165:    slab_rebal.slab_start = NULL;
       86: 1166:    slab_rebal.slab_end   = NULL;
       86: 1167:    slab_rebal.slab_pos   = NULL;
       86: 1168:    evictions_nomem    = slab_rebal.evictions_nomem;
       86: 1169:    inline_reclaim = slab_rebal.inline_reclaim;
       86: 1170:    rescues   = slab_rebal.rescues;
       86: 1171:    chunk_rescues = slab_rebal.chunk_rescues;
       86: 1172:    busy_deletes = slab_rebal.busy_deletes;
       86: 1173:    slab_rebal.evictions_nomem    = 0;
       86: 1174:    slab_rebal.inline_reclaim = 0;
       86: 1175:    slab_rebal.rescues  = 0;
       86: 1176:    slab_rebal.chunk_rescues = 0;
       86: 1177:    slab_rebal.busy_deletes = 0;
        -: 1178:
       86: 1179:    slab_rebalance_signal = 0;
        -: 1180:
       86: 1181:    pthread_mutex_unlock(&slabs_lock);
        -: 1182:
       86: 1183:    STATS_LOCK();
       86: 1184:    stats.slabs_moved++;
       86: 1185:    stats.slab_reassign_rescues += rescues;
       86: 1186:    stats.slab_reassign_evictions_nomem += evictions_nomem;
       86: 1187:    stats.slab_reassign_inline_reclaim += inline_reclaim;
       86: 1188:    stats.slab_reassign_chunk_rescues += chunk_rescues;
       86: 1189:    stats.slab_reassign_busy_deletes += busy_deletes;
       86: 1190:    stats_state.slab_reassign_running = false;
       86: 1191:    STATS_UNLOCK();
        -: 1192:
       86: 1193:    if (settings.verbose > 1) {
    #####: 1194:        fprintf(stderr, "finished a slab move\n");
        -: 1195:    }
       86: 1196:}
        -: 1197:
        -: 1198:/* Slab mover thread.
        -: 1199: * Sits waiting for a condition to jump off and shovel some memory about
        -: 1200: */
       88: 1201:static void *slab_rebalance_thread(void *arg) {
       88: 1202:    int was_busy = 0;
        -: 1203:    /* So we first pass into cond_wait with the mutex held */
       88: 1204:    mutex_lock(&slabs_rebalance_lock);
        -: 1205:
    11135: 1206:    while (do_run_slab_rebalance_thread) {
    11047: 1207:        if (slab_rebalance_signal == 1) {
       87: 1208:            if (slab_rebalance_start() < 0) {
        -: 1209:                /* Handle errors with more specificity as required. */
    #####: 1210:                slab_rebalance_signal = 0;
        -: 1211:            }
        -: 1212:
        -: 1213:            was_busy = 0;
    10960: 1214:        } else if (slab_rebalance_signal && slab_rebal.slab_start != NULL) {
    10872: 1215:            was_busy = slab_rebalance_move();
        -: 1216:        }
        -: 1217:
    11047: 1218:        if (slab_rebal.done) {
       86: 1219:            slab_rebalance_finish();
    10961: 1220:        } else if (was_busy) {
        -: 1221:            /* Stuck waiting for some items to unlock, so slow down a bit
        -: 1222:             * to give them a chance to free up */
      417: 1223:            usleep(1000);
        -: 1224:        }
        -: 1225:
    11046: 1226:        if (slab_rebalance_signal == 0) {
        -: 1227:            /* always hold this lock while we're running */
      174: 1228:            pthread_cond_wait(&slab_rebalance_cond, &slabs_rebalance_lock);
        -: 1229:        }
        -: 1230:    }
    #####: 1231:    return NULL;
        -: 1232:}
        -: 1233:
        -: 1234:/* Iterate at most once through the slab classes and pick a "random" source.
        -: 1235: * I like this better than calling rand() since rand() is slow enough that we
        -: 1236: * can just check all of the classes once instead.
        -: 1237: */
        -: 1238:static int slabs_reassign_pick_any(int dst) {
        -: 1239:    static int cur = POWER_SMALLEST - 1;
    #####: 1240:    int tries = power_largest - POWER_SMALLEST + 1;
    #####: 1241:    for (; tries > 0; tries--) {
    #####: 1242:        cur++;
    #####: 1243:        if (cur > power_largest)
    #####: 1244:            cur = POWER_SMALLEST;
    #####: 1245:        if (cur == dst)
    #####: 1246:            continue;
    #####: 1247:        if (slabclass[cur].slabs > 1) {
        -: 1248:            return cur;
        -: 1249:        }
        -: 1250:    }
        -: 1251:    return -1;
        -: 1252:}
        -: 1253:
       87: 1254:static enum reassign_result_type do_slabs_reassign(int src, int dst) {
       87: 1255:    bool nospare = false;
       87: 1256:    if (slab_rebalance_signal != 0)
        -: 1257:        return REASSIGN_RUNNING;
        -: 1258:
       87: 1259:    if (src == dst)
        -: 1260:        return REASSIGN_SRC_DST_SAME;
        -: 1261:
        -: 1262:    /* Special indicator to choose ourselves. */
       87: 1263:    if (src == -1) {
    #####: 1264:        src = slabs_reassign_pick_any(dst);
        -: 1265:        /* TODO: If we end up back at -1, return a new error type */
        -: 1266:    }
        -: 1267:
       87: 1268:    if (src < SLAB_GLOBAL_PAGE_POOL || src > power_largest ||
       87: 1269:        dst < SLAB_GLOBAL_PAGE_POOL || dst > power_largest)
        -: 1270:        return REASSIGN_BADCLASS;
        -: 1271:
       87: 1272:    pthread_mutex_lock(&slabs_lock);
       87: 1273:    if (slabclass[src].slabs < 2)
    #####: 1274:        nospare = true;
       87: 1275:    pthread_mutex_unlock(&slabs_lock);
       87: 1276:    if (nospare)
        -: 1277:        return REASSIGN_NOSPARE;
        -: 1278:
       87: 1279:    slab_rebal.s_clsid = src;
       87: 1280:    slab_rebal.d_clsid = dst;
        -: 1281:
       87: 1282:    slab_rebalance_signal = 1;
       87: 1283:    pthread_cond_signal(&slab_rebalance_cond);
        -: 1284:
       87: 1285:    return REASSIGN_OK;
        -: 1286:}
        -: 1287:
       87: 1288:enum reassign_result_type slabs_reassign(int src, int dst) {
        -: 1289:    enum reassign_result_type ret;
       87: 1290:    if (pthread_mutex_trylock(&slabs_rebalance_lock) != 0) {
        -: 1291:        return REASSIGN_RUNNING;
        -: 1292:    }
       87: 1293:    ret = do_slabs_reassign(src, dst);
       87: 1294:    pthread_mutex_unlock(&slabs_rebalance_lock);
       87: 1295:    return ret;
        -: 1296:}
        -: 1297:
        -: 1298:/* If we hold this lock, rebalancer can't wake up or move */
    #####: 1299:void slabs_rebalancer_pause(void) {
    #####: 1300:    pthread_mutex_lock(&slabs_rebalance_lock);
    #####: 1301:}
        -: 1302:
    #####: 1303:void slabs_rebalancer_resume(void) {
    #####: 1304:    pthread_mutex_unlock(&slabs_rebalance_lock);
    #####: 1305:}
        -: 1306:
        -: 1307:static pthread_t rebalance_tid;
        -: 1308:
       88: 1309:int start_slab_maintenance_thread(void) {
        -: 1310:    int ret;
       88: 1311:    slab_rebalance_signal = 0;
       88: 1312:    slab_rebal.slab_start = NULL;
       88: 1313:    char *env = getenv("MEMCACHED_SLAB_BULK_CHECK");
       88: 1314:    if (env != NULL) {
    #####: 1315:        slab_bulk_check = atoi(env);
    #####: 1316:        if (slab_bulk_check == 0) {
    #####: 1317:            slab_bulk_check = DEFAULT_SLAB_BULK_CHECK;
        -: 1318:        }
        -: 1319:    }
        -: 1320:
       88: 1321:    if (pthread_cond_init(&slab_rebalance_cond, NULL) != 0) {
    #####: 1322:        fprintf(stderr, "Can't initialize rebalance condition\n");
    #####: 1323:        return -1;
        -: 1324:    }
       88: 1325:    pthread_mutex_init(&slabs_rebalance_lock, NULL);
        -: 1326:
       88: 1327:    if ((ret = pthread_create(&rebalance_tid, NULL,
        -: 1328:                              slab_rebalance_thread, NULL)) != 0) {
    #####: 1329:        fprintf(stderr, "Can't create rebal thread: %s\n", strerror(ret));
    #####: 1330:        return -1;
        -: 1331:    }
        -: 1332:    return 0;
        -: 1333:}
        -: 1334:
        -: 1335:/* The maintenance thread is on a sleep/loop cycle, so it should join after a
        -: 1336: * short wait */
    #####: 1337:void stop_slab_maintenance_thread(void) {
    #####: 1338:    mutex_lock(&slabs_rebalance_lock);
    #####: 1339:    do_run_slab_thread = 0;
    #####: 1340:    do_run_slab_rebalance_thread = 0;
    #####: 1341:    pthread_cond_signal(&slab_rebalance_cond);
    #####: 1342:    pthread_mutex_unlock(&slabs_rebalance_lock);
        -: 1343:
        -: 1344:    /* Wait for the maintenance thread to stop */
    #####: 1345:    pthread_join(rebalance_tid, NULL);
    #####: 1346:}
